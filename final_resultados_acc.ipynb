{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58b81ba",
   "metadata": {},
   "source": [
    "# PAINTINGS AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3544a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga librerías\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a35591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Dataset personalizado\n",
    "class ArtDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.authors = sorted(self.data['artist'].unique())\n",
    "        self.styles = sorted(self.data['style'].unique())\n",
    "        self.author_to_idx = {author: idx for idx, author in enumerate(self.authors)}\n",
    "        self.style_to_idx = {style: idx for idx, style in enumerate(self.styles)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['path'].replace('\\\\', '/')\n",
    "        full_path = os.path.join(self.img_dir, os.path.basename(img_path))\n",
    "        image = Image.open(full_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        author = self.author_to_idx[self.data.iloc[idx]['artist']]\n",
    "        style = self.style_to_idx[self.data.iloc[idx]['style']]\n",
    "        return image, author, style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cbd0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "# Obtener la ruta absoluta del directorio donde está el script .py\n",
    "script_dir = os.path.dirname(os.path.abspath(\"1001_images\"))\n",
    "\n",
    "# Ruta a la carpeta de imágenes que está en el mismo lugar que el script\n",
    "img_dir = os.path.join(script_dir, \"1001_images\")\n",
    "csv_path = \"https://raw.githubusercontent.com/jsantonjag/PaintingsAI/refs/heads/main/data/dataset_completo.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df['path'] = df['path'].apply(lambda x: os.path.join(img_dir, x))\n",
    "\n",
    "full_dataset = ArtDataset(csv_file=csv_path, img_dir=img_dir, transform=transform)\n",
    "\n",
    "# Dividir en train/test pequeño\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d61437",
   "metadata": {},
   "source": [
    "## ResNet18 (ReLu, BatchNorm, Dropout, Optimizer-Adam, Scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4acbb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo multitarea configurable\n",
    "class MultiTaskResNet(nn.Module):\n",
    "    def __init__(self, num_authors, num_styles, BATCHNORM, ACTIVATION_FN, DROPOUT, DROPOUT_PROB, LINK_FN, loss_function):\n",
    "        super(MultiTaskResNet, self).__init__()\n",
    "        \n",
    "        self.loss_FN = loss_function\n",
    "        self.link_FN = LINK_FN\n",
    "        self.activation_FN = ACTIVATION_FN\n",
    "        self.batchNorm = BATCHNORM\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_prob = DROPOUT_PROB\n",
    "        \n",
    "        base_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        in_features = base_model.fc.in_features\n",
    "\n",
    "        layers = [nn.Flatten()]\n",
    "        if self.batchNorm:\n",
    "            layers.append(nn.BatchNorm1d(in_features))\n",
    "            \n",
    "        layers.append(self.wrap_activation(self.activation_FN))\n",
    "        \n",
    "        if self.dropout:\n",
    "            layers.append(nn.Dropout(self.dropout_prob))\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, in_features))\n",
    "        self.shared_head = nn.Sequential(*layers)\n",
    "        self.fc_artist = nn.Linear(in_features, num_authors)\n",
    "        self.fc_style = nn.Linear(in_features, num_styles)\n",
    "\n",
    "    def wrap_activation(self, ACTIVATION_FN):\n",
    "        if ACTIVATION_FN == torch.relu:\n",
    "            return nn.ReLU()\n",
    "        elif ACTIVATION_FN == torch.tanh:\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Función de activación no soportada. Usa torch.relu o torch.tanh.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.shared_head(x)\n",
    "        artist_output = self.link_FN(self.fc_artist(x), dim=1)\n",
    "        style_output = self.link_FN(self.fc_style(x), dim=1)\n",
    "        return artist_output, style_output\n",
    "    \n",
    "    def compute_loss(self, artist_logits, artist_targets, style_logits, style_targets):\n",
    "        return self.loss_FN(artist_logits, artist_targets) + self.loss_FN(style_logits, style_targets)\n",
    "    \n",
    "    \n",
    "def test_model(model, epochs, train_loader, test_loader, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_loss_list, train_acc_artist_list, test_acc_artist_list = [], [], []\n",
    "    train_acc_style_list, test_acc_style_list = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_artist, correct_style, total = 0, 0, 0\n",
    "\n",
    "        for images, artist_labels, style_labels in train_loader:\n",
    "            images, artist_labels, style_labels = images.to(device), artist_labels.to(device), style_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            artist_logits, style_logits = model(images)\n",
    "            loss = model.compute_loss(artist_logits, artist_labels, style_logits, style_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                artist_preds = torch.argmax(artist_logits, dim=1)\n",
    "                style_preds = torch.argmax(style_logits, dim=1)\n",
    "                correct_artist += (artist_preds == artist_labels).sum().item()\n",
    "                correct_style += (style_preds == style_labels).sum().item()\n",
    "                total += images.size(0)\n",
    "\n",
    "        train_loss_list.append(total_loss / len(train_loader))\n",
    "        train_acc_artist_list.append(correct_artist / total)\n",
    "        train_acc_style_list.append(correct_style / total)\n",
    "\n",
    "        # Evaluación en test\n",
    "        model.eval()\n",
    "        correct_artist_test, correct_style_test, total_test = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, artist_labels, style_labels in test_loader:\n",
    "                images, artist_labels, style_labels = images.to(device), artist_labels.to(device), style_labels.to(device)\n",
    "                artist_logits, style_logits = model(images)\n",
    "                artist_preds = torch.argmax(artist_logits, dim=1)\n",
    "                style_preds = torch.argmax(style_logits, dim=1)\n",
    "                correct_artist_test += (artist_preds == artist_labels).sum().item()\n",
    "                correct_style_test += (style_preds == style_labels).sum().item()\n",
    "                total_test += images.size(0)\n",
    "\n",
    "        test_acc_artist_list.append(correct_artist_test / total_test)\n",
    "        test_acc_style_list.append(correct_style_test / total_test)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(train_loader):.4f} - Train Acc Artist: {correct_artist / total:.2%} - Train Acc Style: {correct_style / total:.2%}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Resultados finales de test:\")\n",
    "    print(f\"Test Accuracy - Artista: {test_acc_artist_list[-1]*100:.2f}%\")\n",
    "    print(f\"Test Accuracy - Estilo:  {test_acc_style_list[-1]*100:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    return train_loss_list, train_acc_artist_list, test_acc_artist_list, train_acc_style_list, test_acc_style_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec2dd6",
   "metadata": {},
   "source": [
    "### Comparing activation functions (ReLU VS Tahn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dffd40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 9.1597 - Train Acc Artist: 2.38% - Train Acc Style: 18.75%\n",
      "Epoch 2/10 - Loss: 9.1515 - Train Acc Artist: 3.00% - Train Acc Style: 17.88%\n",
      "Epoch 3/10 - Loss: 9.1508 - Train Acc Artist: 3.50% - Train Acc Style: 17.75%\n",
      "Epoch 4/10 - Loss: 9.1446 - Train Acc Artist: 3.12% - Train Acc Style: 18.62%\n",
      "Epoch 5/10 - Loss: 9.1617 - Train Acc Artist: 3.38% - Train Acc Style: 17.00%\n",
      "Epoch 6/10 - Loss: 9.1702 - Train Acc Artist: 3.12% - Train Acc Style: 16.00%\n",
      "Epoch 7/10 - Loss: 9.1718 - Train Acc Artist: 3.50% - Train Acc Style: 15.50%\n",
      "Epoch 8/10 - Loss: 9.1695 - Train Acc Artist: 2.88% - Train Acc Style: 16.38%\n",
      "Epoch 9/10 - Loss: 9.1617 - Train Acc Artist: 2.88% - Train Acc Style: 17.12%\n",
      "Epoch 10/10 - Loss: 9.1685 - Train Acc Artist: 2.75% - Train Acc Style: 16.62%\n",
      "==================================================\n",
      "Resultados finales de test:\n",
      "Test Accuracy - Artista: 2.99%\n",
      "Test Accuracy - Estilo:  16.92%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ReLU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiTaskResNet(\n",
    "    num_authors=len(full_dataset.authors), \n",
    "    num_styles=len(full_dataset.styles),\n",
    "    BATCHNORM = True, \n",
    "    ACTIVATION_FN = torch.relu, \n",
    "    DROPOUT = True, \n",
    "    DROPOUT_PROB = 0.3, \n",
    "    LINK_FN = torch.softmax, \n",
    "    loss_function = nn.CrossEntropyLoss(),\n",
    ")\n",
    "\n",
    "losses, train_artist_acc, test_artist_acc, train_style_acc, test_style_acc = test_model(\n",
    "    model=model,\n",
    "    epochs=10,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ec01e",
   "metadata": {},
   "source": [
    "### Improving MultiTaskResNet \n",
    "* modifiying compute_loss\n",
    "* adding scheduler\n",
    "* dropout_prob=0.4\n",
    "* removing softmax as link_function due to the loss_function (crossEntropyLoss()) has logsoftmax() in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f46306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo multitarea configurable\n",
    "class MultiTaskResNet_1(nn.Module):\n",
    "    def __init__(self, num_authors, num_styles, BATCHNORM, ACTIVATION_FN, DROPOUT, DROPOUT_PROB, loss_function):\n",
    "        super(MultiTaskResNet_1, self).__init__()\n",
    "        \n",
    "        self.loss_FN = loss_function\n",
    "        self.link_FN = lambda x, dim: x\n",
    "        self.activation_FN = ACTIVATION_FN\n",
    "        self.batchNorm = BATCHNORM\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_prob = DROPOUT_PROB\n",
    "        \n",
    "        base_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        in_features = base_model.fc.in_features\n",
    "\n",
    "        layers = [nn.Flatten()]\n",
    "        if self.batchNorm:\n",
    "            layers.append(nn.BatchNorm1d(in_features))\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, in_features))\n",
    "        layers.append(self.wrap_activation(self.activation_FN))\n",
    "        \n",
    "        if self.dropout:\n",
    "            layers.append(nn.Dropout(self.dropout_prob))\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, in_features))\n",
    "        layers.append(self.wrap_activation(self.activation_FN))\n",
    "        \n",
    "        self.shared_head = nn.Sequential(*layers)\n",
    "        self.fc_artist = nn.Linear(in_features, num_authors)\n",
    "        self.fc_style = nn.Linear(in_features, num_styles)\n",
    "\n",
    "    def wrap_activation(self, ACTIVATION_FN):\n",
    "        if ACTIVATION_FN == torch.relu:\n",
    "            return nn.ReLU()\n",
    "        elif ACTIVATION_FN == torch.tanh:\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Función de activación no soportada. Usa torch.relu o torch.tanh.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.shared_head(x)\n",
    "        artist_output = self.link_FN(self.fc_artist(x), dim=1)\n",
    "        style_output = self.link_FN(self.fc_style(x), dim=1)\n",
    "        return artist_output, style_output\n",
    "    \n",
    "    def compute_loss(self, artist_logits, artist_targets, style_logits, style_targets):\n",
    "        # ponderación opcional si una tarea es más difícil que otra\n",
    "        return 0.6 * self.loss_FN(artist_logits, artist_targets) + 0.4 * self.loss_FN(style_logits, style_targets)\n",
    "    \n",
    "def test_model(model, epochs, train_loader, test_loader, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "    \n",
    "    train_loss_list, train_acc_artist_list, test_acc_artist_list = [], [], []\n",
    "    train_acc_style_list, test_acc_style_list = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_artist, correct_style, total = 0, 0, 0\n",
    "\n",
    "        for images, artist_labels, style_labels in train_loader:\n",
    "            images, artist_labels, style_labels = images.to(device), artist_labels.to(device), style_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            artist_logits, style_logits = model(images)\n",
    "            loss = model.compute_loss(artist_logits, artist_labels, style_logits, style_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                artist_preds = torch.argmax(artist_logits, dim=1)\n",
    "                style_preds = torch.argmax(style_logits, dim=1)\n",
    "                correct_artist += (artist_preds == artist_labels).sum().item()\n",
    "                correct_style += (style_preds == style_labels).sum().item()\n",
    "                total += images.size(0)\n",
    "\n",
    "        #Scheduling learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss_list.append(total_loss / len(train_loader))\n",
    "        train_acc_artist_list.append(correct_artist / total)\n",
    "        train_acc_style_list.append(correct_style / total)\n",
    "\n",
    "        # Evaluación en test\n",
    "        model.eval()\n",
    "        correct_artist_test, correct_style_test, total_test = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, artist_labels, style_labels in test_loader:\n",
    "                images, artist_labels, style_labels = images.to(device), artist_labels.to(device), style_labels.to(device)\n",
    "                artist_logits, style_logits = model(images)\n",
    "                artist_preds = torch.argmax(artist_logits, dim=1)\n",
    "                style_preds = torch.argmax(style_logits, dim=1)\n",
    "                correct_artist_test += (artist_preds == artist_labels).sum().item()\n",
    "                correct_style_test += (style_preds == style_labels).sum().item()\n",
    "                total_test += images.size(0)\n",
    "\n",
    "        test_acc_artist_list.append(correct_artist_test / total_test)\n",
    "        test_acc_style_list.append(correct_style_test / total_test)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(train_loader):.4f} - Train Acc Artist: {correct_artist / total:.2%} - Train Acc Style: {correct_style / total:.2%}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Resultados finales de test:\")\n",
    "    print(f\"Test Accuracy - Artista: {test_acc_artist_list[-1]*100:.2f}%\")\n",
    "    print(f\"Test Accuracy - Estilo:  {test_acc_style_list[-1]*100:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    return train_loss_list, train_acc_artist_list, test_acc_artist_list, train_acc_style_list, test_acc_style_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92294e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 4.7360 - Train Acc Artist: 3.75% - Train Acc Style: 15.75%\n",
      "Epoch 2/10 - Loss: 4.4289 - Train Acc Artist: 4.50% - Train Acc Style: 19.50%\n",
      "Epoch 3/10 - Loss: 4.2898 - Train Acc Artist: 5.62% - Train Acc Style: 18.38%\n",
      "Epoch 4/10 - Loss: 4.1257 - Train Acc Artist: 5.50% - Train Acc Style: 19.88%\n",
      "Epoch 5/10 - Loss: 3.9894 - Train Acc Artist: 6.25% - Train Acc Style: 21.88%\n",
      "Epoch 6/10 - Loss: 3.8906 - Train Acc Artist: 7.25% - Train Acc Style: 23.25%\n",
      "Epoch 7/10 - Loss: 3.7576 - Train Acc Artist: 6.88% - Train Acc Style: 21.62%\n",
      "Epoch 8/10 - Loss: 3.6540 - Train Acc Artist: 7.25% - Train Acc Style: 25.00%\n",
      "Epoch 9/10 - Loss: 3.5791 - Train Acc Artist: 7.50% - Train Acc Style: 26.25%\n",
      "Epoch 10/10 - Loss: 3.3903 - Train Acc Artist: 11.00% - Train Acc Style: 29.88%\n",
      "==================================================\n",
      "Resultados finales de test:\n",
      "Test Accuracy - Artista: 4.48%\n",
      "Test Accuracy - Estilo:  25.37%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiTaskResNet_1(\n",
    "    num_authors=len(full_dataset.authors), \n",
    "    num_styles=len(full_dataset.styles),\n",
    "    BATCHNORM = True, \n",
    "    ACTIVATION_FN = torch.relu, \n",
    "    DROPOUT = True, \n",
    "    DROPOUT_PROB = 0.4,  \n",
    "    loss_function = nn.CrossEntropyLoss(),\n",
    ")\n",
    "\n",
    "losses_1, train_artist_acc_1, test_artist_acc_1, train_style_acc_1, test_style_acc_1 = test_model(\n",
    "    model=model,\n",
    "    epochs=10,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22823f80",
   "metadata": {},
   "source": [
    "### Improving MultiTaskResNet (by using ResNet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8793002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34, ResNet34_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527f0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo multitarea configurable\n",
    "class MultiTaskResNet34(nn.Module):\n",
    "    def __init__(self, num_authors, num_styles, BATCHNORM, ACTIVATION_FN, DROPOUT, DROPOUT_PROB, loss_function):\n",
    "        super(MultiTaskResNet34, self).__init__()\n",
    "        \n",
    "        self.loss_FN = loss_function\n",
    "        self.link_FN = lambda x, dim: x\n",
    "        self.activation_FN = ACTIVATION_FN\n",
    "        self.batchNorm = BATCHNORM\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_prob = DROPOUT_PROB\n",
    "        \n",
    "        base_model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        in_features = base_model.fc.in_features\n",
    "\n",
    "        layers = [nn.Flatten()]\n",
    "        if self.batchNorm:\n",
    "            layers.append(nn.BatchNorm1d(in_features))\n",
    "            \n",
    "        layers += [\n",
    "            nn.Linear(in_features, 512),\n",
    "            self.wrap_activation(self.activation_FN),\n",
    "        ]\n",
    "                \n",
    "        if self.dropout:\n",
    "            layers.append(nn.Dropout(self.dropout_prob))\n",
    "        \n",
    "        layers += [\n",
    "            nn.Linear(512, 256),\n",
    "            self.wrap_activation(self.activation_FN),\n",
    "        ]\n",
    "        \n",
    "        if self.dropout:\n",
    "            layers.append(nn.Dropout(self.dropout_prob))\n",
    "        \n",
    "        layers += [\n",
    "            nn.Linear(256, in_features),\n",
    "            self.wrap_activation(self.activation_FN),\n",
    "        ]\n",
    "        \n",
    "        self.shared_head = nn.Sequential(*layers)\n",
    "        self.fc_artist = nn.Linear(in_features, num_authors)\n",
    "        self.fc_style = nn.Linear(in_features, num_styles)\n",
    "\n",
    "    def wrap_activation(self, ACTIVATION_FN):\n",
    "        if ACTIVATION_FN == torch.relu:\n",
    "            return nn.ReLU()\n",
    "        elif ACTIVATION_FN == torch.tanh:\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Función de activación no soportada. Usa torch.relu o torch.tanh.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.shared_head(x)\n",
    "        artist_output = self.link_FN(self.fc_artist(x), dim=1)\n",
    "        style_output = self.link_FN(self.fc_style(x), dim=1)\n",
    "        return artist_output, style_output\n",
    "    \n",
    "    def compute_loss(self, artist_logits, artist_targets, style_logits, style_targets):\n",
    "        # ponderación opcional si una tarea es más difícil que otra\n",
    "        return 0.6 * self.loss_FN(artist_logits, artist_targets) + 0.4 * self.loss_FN(style_logits, style_targets)\n",
    "    \n",
    "def test_model(model, epochs, train_loader, test_loader, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "    \n",
    "    train_loss_list, train_acc_artist_list, test_acc_artist_list = [], [], []\n",
    "    train_acc_style_list, test_acc_style_list = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_artist, correct_style, total = 0, 0, 0\n",
    "\n",
    "        for images, artist_labels, style_labels in train_loader:\n",
    "            images, artist_labels, style_labels = images.to(device), artist_labels.to(device), style_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            artist_logits, style_logits = model(images)\n",
    "            loss = model.compute_loss(artist_logits, artist_labels, style_logits, style_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                artist_preds = torch.argmax(artist_logits, dim=1)\n",
    "                style_preds = torch.argmax(style_logits, dim=1)\n",
    "                correct_artist += (artist_preds == artist_labels).sum().item()\n",
    "                correct_style += (style_preds == style_labels).sum().item()\n",
    "                total += images.size(0)\n",
    "\n",
    "        #Scheduling learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss_list.append(total_loss / len(train_loader))\n",
    "        train_acc_artist_list.append(correct_artist / total)\n",
    "        train_acc_style_list.append(correct_style / total)\n",
    "\n",
    "        # Evaluación en test\n",
    "        model.eval()\n",
    "        correct_artist_test, correct_style_test, total_test = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, artist_labels, style_labels in test_loader:\n",
    "                images, artist_labels, style_labels = images.to(device), artist_labels.to(device), style_labels.to(device)\n",
    "                artist_logits, style_logits = model(images)\n",
    "                artist_preds = torch.argmax(artist_logits, dim=1)\n",
    "                style_preds = torch.argmax(style_logits, dim=1)\n",
    "                correct_artist_test += (artist_preds == artist_labels).sum().item()\n",
    "                correct_style_test += (style_preds == style_labels).sum().item()\n",
    "                total_test += images.size(0)\n",
    "\n",
    "        test_acc_artist_list.append(correct_artist_test / total_test)\n",
    "        test_acc_style_list.append(correct_style_test / total_test)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(train_loader):.4f} - Train Acc Artist: {correct_artist / total:.2%} - Train Acc Style: {correct_style / total:.2%}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Resultados finales de test:\")\n",
    "    print(f\"Test Accuracy - Artista: {test_acc_artist_list[-1]*100:.2f}%\")\n",
    "    print(f\"Test Accuracy - Estilo:  {test_acc_style_list[-1]*100:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return train_loss_list, train_acc_artist_list, test_acc_artist_list, train_acc_style_list, test_acc_style_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e67988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 4.7573 - Train Acc Artist: 3.25% - Train Acc Style: 13.00%\n",
      "Epoch 2/10 - Loss: 4.5524 - Train Acc Artist: 3.38% - Train Acc Style: 14.50%\n",
      "Epoch 3/10 - Loss: 4.5387 - Train Acc Artist: 3.00% - Train Acc Style: 15.62%\n",
      "Epoch 4/10 - Loss: 4.5072 - Train Acc Artist: 3.38% - Train Acc Style: 16.00%\n",
      "Epoch 5/10 - Loss: 4.4854 - Train Acc Artist: 3.12% - Train Acc Style: 16.88%\n",
      "Epoch 6/10 - Loss: 4.4393 - Train Acc Artist: 3.75% - Train Acc Style: 17.62%\n",
      "Epoch 7/10 - Loss: 4.4133 - Train Acc Artist: 3.50% - Train Acc Style: 17.38%\n",
      "Epoch 8/10 - Loss: 4.3890 - Train Acc Artist: 3.38% - Train Acc Style: 16.25%\n",
      "Epoch 9/10 - Loss: 4.3247 - Train Acc Artist: 3.88% - Train Acc Style: 17.50%\n",
      "Epoch 10/10 - Loss: 4.2484 - Train Acc Artist: 4.38% - Train Acc Style: 18.50%\n",
      "==================================================\n",
      "Resultados finales de test:\n",
      "Test Accuracy - Artista: 3.48%\n",
      "Test Accuracy - Estilo:  17.91%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiTaskResNet34(\n",
    "    num_authors=len(full_dataset.authors), \n",
    "    num_styles=len(full_dataset.styles),\n",
    "    BATCHNORM = True, \n",
    "    ACTIVATION_FN = torch.relu, \n",
    "    DROPOUT = True, \n",
    "    DROPOUT_PROB = 0.4,  \n",
    "    loss_function = nn.CrossEntropyLoss(),\n",
    ")\n",
    "\n",
    "losses_36, train_artist_acc_36, test_artist_acc_36, train_style_acc_36, test_style_acc_36 = test_model(\n",
    "    model=model,\n",
    "    epochs=10,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028ebaa",
   "metadata": {},
   "source": [
    "### Improving ResNet34 \n",
    "* Modifiying dropout_prob (dropout_prob=0.2), resize (224x224), compute_loss (70% loss-artist and 30% loss-style)\n",
    "* Grouping artists with fewer than 10 pictures\n",
    "* Applying argumentations in transformers (rotation, flip, colorjitter)\n",
    "* Balanced classes (with WeightedRandomSampler()): calculating inverse weights, balanced sampling in training with inverse weights.\n",
    "* Adding task-specific heads (self.artist_head & self.style_head)\n",
    "* Modifiying scheduler (\"ReduceLROnPlateau\") para ajustar el learning rate\n",
    "* Adding epochs (epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dae4bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca860ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones\n",
    "transform_ = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Dataset personalizado\n",
    "class ArtDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.authors = sorted(self.data['artist'].unique())\n",
    "        self.styles = sorted(self.data['style'].unique())\n",
    "        self.author_to_idx = {author: idx for idx, author in enumerate(self.authors)}\n",
    "        self.style_to_idx = {style: idx for idx, style in enumerate(self.styles)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['path'].replace('\\\\', '/')\n",
    "        full_path = os.path.join(self.img_dir, os.path.basename(img_path))\n",
    "        image = Image.open(full_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        author = self.author_to_idx[self.data.iloc[idx]['artist']]\n",
    "        style = self.style_to_idx[self.data.iloc[idx]['style']]\n",
    "        return image, author, style\n",
    "\n",
    "# Modelo multitarea configurable\n",
    "class MultiTaskResNet_m(nn.Module):\n",
    "    def __init__(self, num_authors, num_styles, BATCHNORM, ACTIVATION_FN, DROPOUT, DROPOUT_PROB, loss_function):\n",
    "        super(MultiTaskResNet_m, self).__init__()\n",
    "        \n",
    "        self.loss_FN = loss_function\n",
    "        self.link_FN = lambda x, dim: x\n",
    "        self.activation_FN = ACTIVATION_FN\n",
    "        self.batchNorm = BATCHNORM\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_prob = DROPOUT_PROB\n",
    "        \n",
    "        base_model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        in_features = base_model.fc.in_features\n",
    "\n",
    "        layers = [nn.Flatten()]\n",
    "        if self.batchNorm:\n",
    "            layers.append(nn.BatchNorm1d(in_features))\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, in_features))\n",
    "        layers.append(self.wrap_activation(self.activation_FN))\n",
    "        \n",
    "        if self.dropout:\n",
    "            layers.append(nn.Dropout(self.dropout_prob))\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, in_features))\n",
    "        layers.append(self.wrap_activation(self.activation_FN))\n",
    "        \n",
    "        self.shared_head = nn.Sequential(*layers)\n",
    "        self.fc_artist = nn.Linear(in_features, num_authors)\n",
    "        self.artist_head = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_prob)\n",
    "        )\n",
    "        self.style_head = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_prob)\n",
    "        )\n",
    "        self.fc_style = nn.Linear(in_features, num_styles)\n",
    "\n",
    "    def wrap_activation(self, ACTIVATION_FN):\n",
    "        if ACTIVATION_FN == torch.relu:\n",
    "            return nn.ReLU()\n",
    "        elif ACTIVATION_FN == torch.tanh:\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Función de activación no soportada. Usa torch.relu o torch.tanh.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.shared_head(x)\n",
    "        \n",
    "        artist_output = self.link_FN(self.fc_artist(self.artist_head(x)), dim=1)\n",
    "        style_output = self.link_FN(self.fc_style(self.style_head(x)), dim=1)\n",
    "        \n",
    "        return artist_output, style_output\n",
    "    \n",
    "    def compute_loss(self, artist_logits, artist_targets, style_logits, style_targets):\n",
    "        # ponderación opcional si una tarea es más difícil que otra\n",
    "        return 0.7 * self.loss_FN(artist_logits, artist_targets) + 0.3 * self.loss_FN(style_logits, style_targets)\n",
    "    \n",
    "def test_model(model, epochs, train_loader, test_loader, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    train_loss_list, train_acc_artist_list, test_acc_artist_list = [], [], []\n",
    "    train_acc_style_list, test_acc_style_list = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_artist, correct_style, total = 0, 0, 0\n",
    "\n",
    "        for images, artist_labels, style_labels in train_loader:\n",
    "            images, artist_labels, style_labels = images.to(device), artist_labels.to(device), style_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            artist_logits, style_logits = model(images)\n",
    "            loss = model.compute_loss(artist_logits, artist_labels, style_logits, style_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                artist_preds = torch.argmax(artist_logits, dim=1)\n",
    "                style_preds = torch.argmax(style_logits, dim=1)\n",
    "                correct_artist += (artist_preds == artist_labels).sum().item()\n",
    "                correct_style += (style_preds == style_labels).sum().item()\n",
    "                total += images.size(0)\n",
    "\n",
    "               \n",
    "        train_loss_list.append(total_loss / len(train_loader))\n",
    "        train_acc_artist_list.append(correct_artist / total)\n",
    "        train_acc_style_list.append(correct_style / total)\n",
    "\n",
    "        # Evaluación en test\n",
    "        model.eval()\n",
    "        correct_artist_test, correct_style_test, total_test = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, artist_labels, style_labels in test_loader:\n",
    "                images, artist_labels, style_labels = images.to(device), artist_labels.to(device), style_labels.to(device)\n",
    "                artist_logits, style_logits = model(images)\n",
    "                artist_preds = torch.argmax(artist_logits, dim=1)\n",
    "                style_preds = torch.argmax(style_logits, dim=1)\n",
    "                correct_artist_test += (artist_preds == artist_labels).sum().item()\n",
    "                correct_style_test += (style_preds == style_labels).sum().item()\n",
    "                total_test += images.size(0)\n",
    "        \n",
    "        test_acc_artist_list.append(correct_artist_test / total_test)\n",
    "        test_acc_style_list.append(correct_style_test / total_test)\n",
    "\n",
    "        scheduler.step(correct_artist_test/total_test)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(train_loader):.4f} - Train Acc Artist: {correct_artist / total:.2%} - Train Acc Style: {correct_style / total:.2%}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Resultados finales de test:\")\n",
    "    print(f\"Test Accuracy - Artista: {test_acc_artist_list[-1]*100:.2f}%\")\n",
    "    print(f\"Test Accuracy - Estilo:  {test_acc_style_list[-1]*100:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return train_loss_list, train_acc_artist_list, test_acc_artist_list, train_acc_style_list, test_acc_style_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7baad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "script_dir = os.path.dirname(os.path.abspath(\"1001_images\"))\n",
    "img_dir = os.path.join(script_dir, \"1001_images\")\n",
    "csv_path = \"https://raw.githubusercontent.com/jsantonjag/PaintingsAI/refs/heads/main/data/dataset_completo.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "#Agrupar artistas con <10 imágenes como \"Otros\"\n",
    "artist_counts = df['artist'].value_counts()\n",
    "valid_artists = artist_counts[artist_counts >= 10].index.tolist()\n",
    "df['artist'] = df['artist'].apply(lambda x: x if x in valid_artists else 'Otros')\n",
    "\n",
    "df.to_csv(\"filtered_dataset.csv\", index=False)\n",
    "\n",
    "df['path'] = df['path'].apply(lambda x: os.path.join(img_dir, x))\n",
    "\n",
    "full_dataset = ArtDataset(csv_file=\"filtered_dataset.csv\", img_dir=img_dir, transform=transform_)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Calcular pesos inversos por clase\n",
    "author_labels = [sample[1] for sample in train_dataset]\n",
    "class_sample_counts = pd.Series(author_labels).value_counts().sort_index()\n",
    "weights = 1. / class_sample_counts\n",
    "sample_weights = [weights[label] for label in author_labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9726b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 2.3403 - Train Acc Artist: 22.00% - Train Acc Style: 37.62%\n",
      "Epoch 2/20 - Loss: 2.1987 - Train Acc Artist: 25.00% - Train Acc Style: 39.25%\n",
      "Epoch 3/20 - Loss: 2.2069 - Train Acc Artist: 22.75% - Train Acc Style: 38.12%\n",
      "Epoch 4/20 - Loss: 2.0347 - Train Acc Artist: 28.38% - Train Acc Style: 38.75%\n",
      "Epoch 5/20 - Loss: 1.9232 - Train Acc Artist: 33.12% - Train Acc Style: 47.88%\n",
      "Epoch 6/20 - Loss: 1.8718 - Train Acc Artist: 35.12% - Train Acc Style: 49.12%\n",
      "Epoch 7/20 - Loss: 1.8299 - Train Acc Artist: 36.62% - Train Acc Style: 49.88%\n",
      "Epoch 8/20 - Loss: 1.5828 - Train Acc Artist: 43.88% - Train Acc Style: 56.75%\n",
      "Epoch 9/20 - Loss: 1.5297 - Train Acc Artist: 47.38% - Train Acc Style: 56.88%\n",
      "Epoch 10/20 - Loss: 1.3384 - Train Acc Artist: 52.38% - Train Acc Style: 62.00%\n",
      "Epoch 11/20 - Loss: 1.2891 - Train Acc Artist: 56.12% - Train Acc Style: 64.50%\n",
      "Epoch 12/20 - Loss: 1.2284 - Train Acc Artist: 57.50% - Train Acc Style: 65.62%\n",
      "Epoch 13/20 - Loss: 1.0735 - Train Acc Artist: 62.00% - Train Acc Style: 72.00%\n",
      "Epoch 14/20 - Loss: 1.0016 - Train Acc Artist: 65.75% - Train Acc Style: 72.25%\n",
      "Epoch 15/20 - Loss: 0.9529 - Train Acc Artist: 70.25% - Train Acc Style: 70.50%\n",
      "Epoch 16/20 - Loss: 0.7876 - Train Acc Artist: 75.25% - Train Acc Style: 76.25%\n",
      "Epoch 17/20 - Loss: 0.7586 - Train Acc Artist: 76.38% - Train Acc Style: 75.00%\n",
      "Epoch 18/20 - Loss: 0.6818 - Train Acc Artist: 77.88% - Train Acc Style: 78.62%\n",
      "Epoch 19/20 - Loss: 0.6342 - Train Acc Artist: 80.62% - Train Acc Style: 78.50%\n",
      "Epoch 20/20 - Loss: 0.6354 - Train Acc Artist: 80.75% - Train Acc Style: 79.12%\n",
      "==================================================\n",
      "Resultados finales de test:\n",
      "Test Accuracy - Artista: 16.92%\n",
      "Test Accuracy - Estilo:  22.39%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento con la ReLU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiTaskResNet_m(\n",
    "    num_authors=len(full_dataset.authors), \n",
    "    num_styles=len(full_dataset.styles),\n",
    "    BATCHNORM = True, \n",
    "    ACTIVATION_FN = torch.relu, \n",
    "    DROPOUT = True, \n",
    "    DROPOUT_PROB = 0.2,  \n",
    "    loss_function = nn.CrossEntropyLoss(),\n",
    ")\n",
    "\n",
    "losses_n, train_artist_acc_n, test_artist_acc_n, train_style_acc_n, test_style_acc_n = test_model(\n",
    "    model=model,\n",
    "    epochs=20,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
